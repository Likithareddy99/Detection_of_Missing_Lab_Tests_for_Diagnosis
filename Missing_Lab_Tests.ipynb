{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Likithareddy99/Missing_lab_tests/blob/main/Missing_Lab_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D-WNRhg7MnM6",
        "outputId": "fde97df4-6332-4f6f-b68f-7840132ae1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers datasets\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z531eUbFPOsv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from google.colab import files\n",
        "import json\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hvEQsPRmWXmo"
      },
      "outputs": [],
      "source": [
        "#code to generate structured data using named-entity recognition \n",
        "\n",
        "def MissingTests(df1, df_icd, predicted_label):\n",
        "\n",
        "  #initializing bert model \n",
        "  \n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")\n",
        "  model = AutoModelForTokenClassification.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")\n",
        "  Bert_ner = pipeline('ner',model=model,tokenizer=tokenizer,grouped_entities=True,ignore_subwords=True)\n",
        "\n",
        "  result = [] #nested list having dictionary \n",
        "\n",
        "  #Working on sample data of first 1000 rows only\n",
        "  for i in df1['LONG_TITLE'][:1000]:\n",
        "    result.append(Bert_ner(i))\n",
        "\n",
        "  entities=[]\n",
        "  for i in result:\n",
        "    for j in i:\n",
        "      entities.append([j['entity_group'],j['word']])\n",
        "\n",
        "  Entity_table = pd.DataFrame(entities,columns=['entity_type','entity'])   \n",
        "  Entity_table = Entity_table.drop_duplicates()\n",
        "\n",
        "  Entity_table.reset_index(drop=True, inplace=True)\n",
        "  Entity_table.reset_index()\n",
        "\n",
        "\n",
        "\n",
        "  #Processing complete data requires more time and processing power. So, we're using already processed data \n",
        "  result_test={}\n",
        "  for i in range(len(predicted_label)):\n",
        "    for j in range(len(df_icd)):\n",
        "      if df_icd['ICD_code'][j].__contains__(predicted_label[i]):\n",
        "        result_test[df_icd['Entity'][j]]=df_icd[\"test\"][j]\n",
        "  print(\"\\n complete tests required to identify the disease\", result_test)\n",
        "  \n",
        "  #to store using json file format \n",
        "  filename = 'result.json'          #use the file extension .json\n",
        "  with open(filename, 'w') as file_object:  #open the file in write mode\n",
        "    json.dump(result_test, file_object) \n",
        "\n",
        "  return result_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def AutomaticDiag(input):\n",
        "\n",
        "  #Model Initialization\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"bvanaken/CORe-clinical-diagnosis-prediction\")\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"bvanaken/CORe-clinical-diagnosis-prediction\")\n",
        "\n",
        "  tokenized_input = tokenizer(input, return_tensors=\"pt\")\n",
        "  output = model(**tokenized_input)\n",
        "\n",
        "  predictions = torch.sigmoid(output.logits)\n",
        "  predicted_labels = [model.config.id2label[_id] for _id in (predictions > 0.3).nonzero()[:, 1].tolist()] \n",
        "  \n",
        "\n",
        "  #code to check what tests were performed on the patient \n",
        "  \n",
        "  tokenizer_ner = AutoTokenizer.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")\n",
        "  model_ner = AutoModelForTokenClassification.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")\n",
        "  Bert_ner = pipeline('ner',model=model_ner,tokenizer=tokenizer_ner,grouped_entities=True,ignore_subwords=True)   \n",
        "  result = Bert_ner(input)\n",
        "  entities=[]\n",
        "  for j in result:\n",
        "    entities.append([j['entity_group'],j['word']])\n",
        "\n",
        "  Entity_table = pd.DataFrame(entities,columns=['entity_type','entity'])  \n",
        "\n",
        "  Entity_table = Entity_table.drop_duplicates()\n",
        "  Entity_table.reset_index(drop=True, inplace=True)\n",
        "  Entity_table.reset_index()\n",
        "\n",
        "  tests_performed = []\n",
        "  for i in range(len(Entity_table['entity_type'])):\n",
        "    if Entity_table['entity_type'][i] == 'test':\n",
        "      tests_performed.append(Entity_table['entity'][i])\n",
        "\n",
        "  tests_performed #storing in global variable to find missing lab tests by comparision \n",
        "  return predicted_labels, tests_performed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#function for coparing the results \n",
        "\n",
        "def comparision(test_performed, test_required):\n",
        "\n",
        "  list3 = []\n",
        "  suggested_tests = []\n",
        "\n",
        "  for key in test_required:\n",
        "    str1 = test_required[key].replace('[','')\n",
        "    str2 = str1.replace(']','')\n",
        "\n",
        "    list1 = str2.split(\"'\")\n",
        "    new_list = list(filter(lambda item: len(item) >= 1, list1))\n",
        "    list3+=new_list\n",
        "    \n",
        "    list2 = list3\n",
        "    for ele in list3:\n",
        "      if  ele == ' ' or \"\\\\\" in r\"%r\" % ele:\n",
        "        list2.remove(ele)\n",
        "\n",
        "  tests_required2 = list2\n",
        "  for elem in test_performed:\n",
        "    if elem in list2:\n",
        "      tests_required2.remove(elem)\n",
        "\n",
        "  return tests_required2\n",
        "\n",
        "\n",
        "\n",
        "def main(user_input):\n",
        "  Tests_performed1=[]\n",
        "  predicted_label1=[]\n",
        "  test_required1 = {}\n",
        "\n",
        "  \n",
        "  df1 = pd.read_csv('https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/D_ICD_DIAGNOSES.csv')\n",
        "  \n",
        "\n",
        "  df2 = pd.read_csv('https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/Disease_icd.csv')\n",
        "\n",
        "\n",
        "  inp = user_input\n",
        "\n",
        "  #predicted label which we got from the automatic diagnsis \n",
        "  # predicted_label1=['427','482','790']\n",
        "  predicted_label1, Tests_performed1 = AutomaticDiag(inp)   \n",
        "  #passing the predicted label as argument to the Missing lab test \n",
        "  test_required1 = MissingTests(df1,df2, predicted_label1)\n",
        "  print(\"\\n the tests performed on patients are;\",Tests_performed1)\n",
        "\n",
        "  print(\"\\n\\n The missing lab tests to be performed are: \\n\",comparision(Tests_performed1, test_required1))\n",
        "    \n",
        "#Function Call \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lwxz46CuW46R",
        "outputId": "4b791ac8-26fb-4c0c-f500-ebf40e52626c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "ngrokToken = '2FV2D230ltMQHRjIdqBMJyB5JX4_2GXkSrxcqpmgdrYPTPeWg'\n",
        "ngrokTokenCmd = 'ngrok authtoken '+ngrokToken\n",
        "\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K0eSXQtKXBC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f088f152-439a-4d7f-b92d-b083b6e88774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘templates’: File exists\n",
            "mkdir: cannot create directory ‘static’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir templates\n",
        "!mkdir static\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/form.html\n",
        "!wget -q https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/result.html\n",
        "!wget -q https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/app.css\n",
        "!wget -q https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/search.png\n",
        "!wget -q https://raw.githubusercontent.com/Likithareddy99/Missing_lab_tests/main/bg_1.png\n",
        "\n",
        "!mv *.html templates/\n",
        "!mv *.css static/\n",
        "!mv *.png static/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DnkoJ2qdXByI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2fcdae-9e5c-40a0-c596-2be1d596f08a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://184c-34-68-191-214.ngrok.io\" -> \"http://localhost:5000\">"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "get_ipython().system_raw(ngrokTokenCmd)\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bgNXi39NXDVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8dcb4a-46ce-4180-cb32-1e2717aa64a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Sep/2022 18:59:28] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Sep/2022 18:59:29] \"\u001b[33mGET /static/bootstrap.min.css HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Sep/2022 18:59:29] \"\u001b[37mGET /static/app.css HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Sep/2022 18:59:29] \"\u001b[37mGET /static/search.png HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Sep/2022 18:59:29] \"\u001b[37mGET /static/bg_1.png HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Sep/2022 18:59:29] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "from sys import argv\n",
        "from flask import Flask\n",
        "from flask import Flask, render_template, redirect, url_for, request\n",
        "app = Flask(__name__)\n",
        "\n",
        "\"\"\"@app.route(\"/\")\n",
        "def hello():\n",
        "    retString = 'Hello visitor'\n",
        "    return retString\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def form():\n",
        "    return render_template('form.html')\n",
        " \n",
        "@app.route('/data/', methods = ['POST', 'GET'])\n",
        "def data():\n",
        "    if request.method == 'GET':\n",
        "        return f\"The URL /data is accessed directly. Try going to '/form' to submit form\"\n",
        "    if request.method == 'POST':\n",
        "        form_data = request.form\n",
        "        final_data = main(form_data['Input'])\n",
        "        print(final_data)\n",
        "        return render_template('result.html',result = form_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}